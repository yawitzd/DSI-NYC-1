{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Spark Overview\n",
    "Week 10 | Lesson 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- Describe Spark, how it works and its role in a Hadoop ecosystem\n",
    "- Identify Spark use cases\n",
    "- Run simple queries in Spark from the command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STUDENT PRE-WORK\n",
    "*Before this lesson, you should already be able to:*\n",
    "- Perform MapReduce jobs with Hadoop\n",
    "- Perform MapReduce with Python MRJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LESSON GUIDE\n",
    "| TIMING  | TYPE  | TOPIC  |\n",
    "|:-:|---|---|\n",
    "| 5 min | [Opening](#opening) | Opening |\n",
    "| 20 min | [Introduction](#introduction) | Intro to Spark |\n",
    "| 10 min | [Introduction2](#introduction2) | Spark Stack and API |\n",
    "| 15 min | [Demo](#demo) | Demo: Spark Map Reduce |\n",
    "| 10 min | [Guided-practice](#guided-practice) | Guided Practice: Spark Map Reduce |\n",
    "| 15 min | [Ind-practice](#ind-practice) | Independent Practice: Explore the Spark Shell |\n",
    "| 10 min | [Conclusion](#conclusion) | Conclusion |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"opening\"></a>\n",
    "## Opening (5 min)\n",
    "\n",
    "Spark, a cluster computing framework, has gained tremendous ground in industry since its widescale release several years ago. It can run on Hadoop as a replacement to MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./assets/images/indeed-hadoop-spark.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Check:** what's MapReduce?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> It's a framework -- and a specific implementation on Hadoop -- to solve problems that involve parallel calculation. It's composed of 2 steps: a mapper step in which multiple workers solve the same task on different parts of the dataset and a reducer phase where the results of each workers' work are combined to give a final result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Check:** what limitations have you encountered when processing data with Hadoop and MapReduce?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Mainly performance. It takes a long time to perform (and sometimes to write) a MapReduce job, which makes it really hard to experiment and iterate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-means in Java on Hadoop\n",
    "\n",
    "```java\n",
    "/*\n",
    " * @author Himank Chaudhary\n",
    " */\n",
    "\n",
    "import java.io.IOException;\n",
    "import java.util.*;\n",
    "import java.io.*;\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.filecache.DistributedCache;\n",
    "import org.apache.hadoop.fs.FileSystem;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.*;\n",
    "import org.apache.hadoop.mapred.*;\n",
    "import org.apache.hadoop.mapred.Reducer;\n",
    "\n",
    "@SuppressWarnings(\"deprecation\")\n",
    "public class KMeans {\n",
    "\tpublic static String OUT = \"outfile\";\n",
    "\tpublic static String IN = \"inputlarger\";\n",
    "\tpublic static String CENTROID_FILE_NAME = \"/centroid.txt\";\n",
    "\tpublic static String OUTPUT_FILE_NAME = \"/part-00000\";\n",
    "\tpublic static String DATA_FILE_NAME = \"/data.txt\";\n",
    "\tpublic static String JOB_NAME = \"KMeans\";\n",
    "\tpublic static String SPLITTER = \"\\t| \";\n",
    "\tpublic static List<Double> mCenters = new ArrayList<Double>();\n",
    "\n",
    "\t/*\n",
    "\t * In Mapper class we are overriding configure function. In this we are\n",
    "\t * reading file from Distributed Cache and then storing that into instance\n",
    "\t * variable \"mCenters\"\n",
    "\t */\n",
    "\tpublic static class Map extends MapReduceBase implements\n",
    "\t\t\tMapper<LongWritable, Text, DoubleWritable, DoubleWritable> {\n",
    "\t\t@Override\n",
    "\t\tpublic void configure(JobConf job) {\n",
    "\t\t\ttry {\n",
    "\t\t\t\t// Fetch the file from Distributed Cache Read it and store the\n",
    "\t\t\t\t// centroid in the ArrayList\n",
    "\t\t\t\tPath[] cacheFiles = DistributedCache.getLocalCacheFiles(job);\n",
    "\t\t\t\tif (cacheFiles != null && cacheFiles.length > 0) {\n",
    "\t\t\t\t\tString line;\n",
    "\t\t\t\t\tmCenters.clear();\n",
    "\t\t\t\t\tBufferedReader cacheReader = new BufferedReader(\n",
    "\t\t\t\t\t\t\tnew FileReader(cacheFiles[0].toString()));\n",
    "\t\t\t\t\ttry {\n",
    "\t\t\t\t\t\t// Read the file split by the splitter and store it in\n",
    "\t\t\t\t\t\t// the list\n",
    "\t\t\t\t\t\twhile ((line = cacheReader.readLine()) != null) {\n",
    "\t\t\t\t\t\t\tString[] temp = line.split(SPLITTER);\n",
    "\t\t\t\t\t\t\tmCenters.add(Double.parseDouble(temp[0]));\n",
    "\t\t\t\t\t\t}\n",
    "\t\t\t\t\t} finally {\n",
    "\t\t\t\t\t\tcacheReader.close();\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t}\n",
    "\t\t\t} catch (IOException e) {\n",
    "\t\t\t\tSystem.err.println(\"Exception reading DistribtuedCache: \" + e);\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\n",
    "\t\t/*\n",
    "\t\t * Map function will find the minimum center of the point and emit it to\n",
    "\t\t * the reducer\n",
    "\t\t */\n",
    "\t\t@Override\n",
    "\t\tpublic void map(LongWritable key, Text value,\n",
    "\t\t\t\tOutputCollector<DoubleWritable, DoubleWritable> output,\n",
    "\t\t\t\tReporter reporter) throws IOException {\n",
    "\t\t\tString line = value.toString();\n",
    "\t\t\tdouble point = Double.parseDouble(line);\n",
    "\t\t\tdouble min1, min2 = Double.MAX_VALUE, nearest_center = mCenters\n",
    "\t\t\t\t\t.get(0);\n",
    "\t\t\t// Find the minimum center from a point\n",
    "\t\t\tfor (double c : mCenters) {\n",
    "\t\t\t\tmin1 = c - point;\n",
    "\t\t\t\tif (Math.abs(min1) < Math.abs(min2)) {\n",
    "\t\t\t\t\tnearest_center = c;\n",
    "\t\t\t\t\tmin2 = min1;\n",
    "\t\t\t\t}\n",
    "\t\t\t}\n",
    "\t\t\t// Emit the nearest center and the point\n",
    "\t\t\toutput.collect(new DoubleWritable(nearest_center),\n",
    "\t\t\t\t\tnew DoubleWritable(point));\n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "\tpublic static class Reduce extends MapReduceBase implements\n",
    "\t\t\tReducer<DoubleWritable, DoubleWritable, DoubleWritable, Text> {\n",
    "\n",
    "\t\t/*\n",
    "\t\t * Reduce function will emit all the points to that center and calculate\n",
    "\t\t * the next center for these points\n",
    "\t\t */\n",
    "\t\t@Override\n",
    "\t\tpublic void reduce(DoubleWritable key, Iterator<DoubleWritable> values,\n",
    "\t\t\t\tOutputCollector<DoubleWritable, Text> output, Reporter reporter)\n",
    "\t\t\t\tthrows IOException {\n",
    "\t\t\tdouble newCenter;\n",
    "\t\t\tdouble sum = 0;\n",
    "\t\t\tint no_elements = 0;\n",
    "\t\t\tString points = \"\";\n",
    "\t\t\twhile (values.hasNext()) {\n",
    "\t\t\t\tdouble d = values.next().get();\n",
    "\t\t\t\tpoints = points + \" \" + Double.toString(d);\n",
    "\t\t\t\tsum = sum + d;\n",
    "\t\t\t\t++no_elements;\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\t// We have new center now\n",
    "\t\t\tnewCenter = sum / no_elements;\n",
    "\n",
    "\t\t\t// Emit new center and point\n",
    "\t\t\toutput.collect(new DoubleWritable(newCenter), new Text(points));\n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "\tpublic static void main(String[] args) throws Exception {\n",
    "\t\trun(args);\n",
    "\t}\n",
    "\n",
    "\tpublic static void run(String[] args) throws Exception {\n",
    "\t\tIN = args[0];\n",
    "\t\tOUT = args[1];\n",
    "\t\tString input = IN;\n",
    "\t\tString output = OUT + System.nanoTime();\n",
    "\t\tString again_input = output;\n",
    "\n",
    "\t\t// Reiterating till the convergence\n",
    "\t\tint iteration = 0;\n",
    "\t\tboolean isdone = false;\n",
    "\t\twhile (isdone == false) {\n",
    "\t\t\tJobConf conf = new JobConf(KMeans.class);\n",
    "\t\t\tif (iteration == 0) {\n",
    "\t\t\t\tPath hdfsPath = new Path(input + CENTROID_FILE_NAME);\n",
    "\t\t\t\t// upload the file to hdfs. Overwrite any existing copy.\n",
    "\t\t\t\tDistributedCache.addCacheFile(hdfsPath.toUri(), conf);\n",
    "\t\t\t} else {\n",
    "\t\t\t\tPath hdfsPath = new Path(again_input + OUTPUT_FIE_NAME);\n",
    "\t\t\t\t// upload the file to hdfs. Overwrite any existing copy.\n",
    "\t\t\t\tDistributedCache.addCacheFile(hdfsPath.toUri(), conf);\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\tconf.setJobName(JOB_NAME);\n",
    "\t\t\tconf.setMapOutputKeyClass(DoubleWritable.class);\n",
    "\t\t\tconf.setMapOutputValueClass(DoubleWritable.class);\n",
    "\t\t\tconf.setOutputKeyClass(DoubleWritable.class);\n",
    "\t\t\tconf.setOutputValueClass(Text.class);\n",
    "\t\t\tconf.setMapperClass(Map.class);\n",
    "\t\t\tconf.setReducerClass(Reduce.class);\n",
    "\t\t\tconf.setInputFormat(TextInputFormat.class);\n",
    "\t\t\tconf.setOutputFormat(TextOutputFormat.class);\n",
    "\n",
    "\t\t\tFileInputFormat.setInputPaths(conf,\n",
    "\t\t\t\t\tnew Path(input + DATA_FILE_NAME));\n",
    "\t\t\tFileOutputFormat.setOutputPath(conf, new Path(output));\n",
    "\n",
    "\t\t\tJobClient.runJob(conf);\n",
    "\n",
    "\t\t\tPath ofile = new Path(output + OUTPUT_FIE_NAME);\n",
    "\t\t\tFileSystem fs = FileSystem.get(new Configuration());\n",
    "\t\t\tBufferedReader br = new BufferedReader(new InputStreamReader(\n",
    "\t\t\t\t\tfs.open(ofile)));\n",
    "\t\t\tList<Double> centers_next = new ArrayList<Double>();\n",
    "\t\t\tString line = br.readLine();\n",
    "\t\t\twhile (line != null) {\n",
    "\t\t\t\tString[] sp = line.split(\"\\t| \");\n",
    "\t\t\t\tdouble c = Double.parseDouble(sp[0]);\n",
    "\t\t\t\tcenters_next.add(c);\n",
    "\t\t\t\tline = br.readLine();\n",
    "\t\t\t}\n",
    "\t\t\tbr.close();\n",
    "\n",
    "\t\t\tString prev;\n",
    "\t\t\tif (iteration == 0) {\n",
    "\t\t\t\tprev = input + CENTROID_FILE_NAME;\n",
    "\t\t\t} else {\n",
    "\t\t\t\tprev = again_input + OUTPUT_FILE_NAME;\n",
    "\t\t\t}\n",
    "\t\t\tPath prevfile = new Path(prev);\n",
    "\t\t\tFileSystem fs1 = FileSystem.get(new Configuration());\n",
    "\t\t\tBufferedReader br1 = new BufferedReader(new InputStreamReader(\n",
    "\t\t\t\t\tfs1.open(prevfile)));\n",
    "\t\t\tList<Double> centers_prev = new ArrayList<Double>();\n",
    "\t\t\tString l = br1.readLine();\n",
    "\t\t\twhile (l != null) {\n",
    "\t\t\t\tString[] sp1 = l.split(SPLITTER);\n",
    "\t\t\t\tdouble d = Double.parseDouble(sp1[0]);\n",
    "\t\t\t\tcenters_prev.add(d);\n",
    "\t\t\t\tl = br1.readLine();\n",
    "\t\t\t}\n",
    "\t\t\tbr1.close();\n",
    "\n",
    "\t\t\t// Sort the old centroid and new centroid and check for convergence\n",
    "\t\t\t// condition\n",
    "\t\t\tCollections.sort(centers_next);\n",
    "\t\t\tCollections.sort(centers_prev);\n",
    "\n",
    "\t\t\tIterator<Double> it = centers_prev.iterator();\n",
    "\t\t\tfor (double d : centers_next) {\n",
    "\t\t\t\tdouble temp = it.next();\n",
    "\t\t\t\tif (Math.abs(temp - d) <= 0.1) {\n",
    "\t\t\t\t\tisdone = true;\n",
    "\t\t\t\t} else {\n",
    "\t\t\t\t\tisdone = false;\n",
    "\t\t\t\t\tbreak;\n",
    "\t\t\t\t}\n",
    "\t\t\t}\n",
    "\t\t\t++iteration;\n",
    "\t\t\tagain_input = output;\n",
    "\t\t\toutput = OUT + System.nanoTime();\n",
    "\t\t}\n",
    "\t}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In Python with MRJob\n",
    "\n",
    "```python\n",
    "from mrjob.job import MRJob\n",
    "import mrjob\n",
    "# MRJob is a python class which will be overloaded\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "class MRKMeans(MRJob):\n",
    "\n",
    "\t SORT_VALUES = True\n",
    "\t OUTPUT_PROTOCOL = mrjob.protocol.RawProtocol\n",
    "\t \n",
    "\t def dist_vec(self,v1,v2):\n",
    "\t\t #calculate the distance between two vectors (in two dimensions)\n",
    "\t\t return sqrt((v2[0]-v1[0])*(v2[0]-v1[0])+(v2[1]-v1[1])*(v2[1]-v1[1]))\n",
    "\t \n",
    "\t def configure_options(self):\n",
    "\t\t super(MRKMeans, self).configure_options()\n",
    "\t\t #the line below define that the file folowing the --c option is the centroid and is loadable\n",
    "\t\t self.add_file_option('--c')\n",
    "\n",
    "\t def get_centroids(self):\n",
    "\t\t \"\"\"\n",
    "\t\t Definition : extracts centroids from the centroids file define afetr --c flag\n",
    "\t\t Out : Return the list of centroids\n",
    "\t\t \"\"\"\n",
    "\t\t # self.options.c is the name of the file following --c option\n",
    "\t\t f = open(self.options.c,'r')\n",
    "\t\t centroids=[]\n",
    "\t\t for line in f.read().split('\\n'):\n",
    "\t\t if line:\n",
    "\t\t x,y = line.split(',')\n",
    "\t\t centroids.append([float(x),float(y)])\n",
    "\t\t f.close()\n",
    "\t\t return centroids\n",
    "\t \n",
    "\t def mapper(self, _, lines):\n",
    "\t\t \"\"\"\n",
    "\t\t Definition : Mapper take centroids extract form get_centroids() and the point cloud and for each point, calculate the distance to the centroids, find the mininum of it\n",
    "\t\t Out : yield the point with it's class\n",
    "\t\t \"\"\"\n",
    "\t\t centroids = self.get_centroids()\n",
    "\t\t for l in lines.split('\\n x,y = l.split(',')\n",
    "\t\t point = [float(x),float(y)]\n",
    "\t\t min_dist=100000000.0\n",
    "\t\t classe = 0\n",
    "\t\t #iterate over the centroids (Here we know that we are doing a 3means)\n",
    "\t\t for i in range(3):\n",
    "\t\t dist = self.dist_vec(point,centroids[i])\n",
    "\t\t if dist < min_dist:\n",
    "\t\t min_dist = dist\n",
    "\t\t classe = i\n",
    "\t\t yield classe, point\n",
    "\t \n",
    "\t def combiner(self,k,v):\n",
    "\t\t \"\"\"\n",
    "\t\t Definition : Calculate for each class, at the end of the mapper, before reducer, the medium point of each class\n",
    "\t\t Out: return for each class, the centroids for each mapper\n",
    "\t\t \"\"\"\n",
    "\t\t count = 0\n",
    "\t\t moy_x=moy_y=0.0\n",
    "\t\t for t in v:\n",
    "\t\t count += 1\n",
    "\t\t moy_x+=t[0]\n",
    "\t\t moy_y+=t[1]\n",
    "\t\t yield k, (moy_x/count,moy_y/count)\n",
    "\n",
    "\t def reducer(self, k, v):\n",
    "\t\t \"\"\"\n",
    "\t\t Definition : for each class, get all the tmp centroids from each combiner and calculate the new centroids.\n",
    "\t\t \"\"\"\n",
    "\t\t # k is class and v are medium points linked to the class\n",
    "\t\t count = 0\n",
    "\t\t moy_x=moy_y=0.0\n",
    "\t\t for t in v:\n",
    "\t\t count += 1\n",
    "\t\t moy_x+=t[0]\n",
    "\t\t moy_y+=t[1]\n",
    "\t\t print str(k)+\",\"+str(moy_x/count)+\",\"+str(moy_y/count)\n",
    " \n",
    "if __name__ == '__main__':\n",
    " #just run mapreduce !\n",
    " MRKMeans.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### In Python with the Spark API.\n",
    "\n",
    "```python\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "from math import sqrt\n",
    "\n",
    "# Load and parse the data\n",
    "data = sc.textFile(\"kmeans_data.txt\")\n",
    "parsedData = data.map(lambda line: [float(x) for x in line.split(' ')])\n",
    "\n",
    "# Build the model (cluster the data)\n",
    "clusters = KMeans.train(parsedData, 2, maxIterations=10,\n",
    " runs=10, initializationMode=\"random\")\n",
    "\n",
    "print clusters.clusterCenters\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    " center = clusters.centers[clusters.predict(point)]\n",
    " return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"introduction\"></a>\n",
    "## Intro to Spark (20 min)\n",
    "\n",
    "Spark was developed in response to limitations in the MapReduce cluster computing paradigm. In MapReducem data is read from disk and then a function is mapped across the data. Then the reducer will reduce the results of the map and finally store reduction results back to HDFS. Spark relaxes the constraints of MR by doing the following:\n",
    "\n",
    "- Generalizes computation from MapReduce-only graphs to arbitrary Directed Acyclic Graphs (DAGs)\n",
    "- Removes a lot of boilerplate code present in Hadoop\n",
    "- Allows tweaks to otherwise inaccesible components in Hadoop, such as the sort algorithm\n",
    "- Loads data into cluster memory, rather than reading from disk, speeding up I/O enormously\n",
    "\n",
    "This makes Spark vastly preferable for certain use cases, especially ingesting streaming data, doing real-time interactive analytics and running machine learning algorithms.\n",
    "\n",
    "The two pillars on which Spark is based are RDDs and DAGs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Resilient Distributed Datasets (RDDs)\n",
    "\n",
    "Spark is based on a data structure called _resilient distributed datasets (RDD)_, an object that represents data placed into the system. The critical improvement here is that data can reside in-memory. The two key RDD concepts are:\n",
    "\n",
    "- Transformations, doing something to RDDs to produce new RDDs (e.g. filtering)\n",
    "- Actions, asking Spark to process an RDD to output some result (e.g. counting things)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./assets/images/spark-rdd.png)\n",
    "(Image from https://dzone.com/refcardz/apache-spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Directed Acyclic Graph\n",
    "\n",
    "The DAG (directed acyclic graph) model is not unlike a Git commit history. \n",
    "\n",
    "Each node in the graph represent a particular operation on the data. The graph is _directed_, meaning the information only flows in one direction along the edges and it cannot flow backwards. (Or in circles, hence _acyclic_.)  This is makes the identification of inputs and outputs easy and unique. This is good for fault tolerance: when a system failure wrecks an RDD, the Spark engine can trace its lineage to recreate it.\n",
    "\n",
    "![DAG](./assets/images/dag.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"introduction2\"></a>\n",
    "## Spark Stack and API (10 min)\n",
    "\n",
    "![](./assets/images/spark-stack.png)\n",
    "\n",
    "The Spark Core is the foundation of the overall ecosystem. It provides distributed task dispatching, scheduling, and basic I/O functionalities, exposed through an application programming interface (for Java, Python, Scala, and R) centered on the RDD abstraction. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here are some of the operations offered by the Spark API:\n",
    "\n",
    "- map\n",
    "- filter\n",
    "- groupby\n",
    "- union\n",
    "- sort\n",
    "- join\n",
    "- reduce\n",
    "- count\n",
    "- fold\n",
    "- reduceByKey\n",
    "- cogroup\n",
    "- cross\n",
    "- zip\n",
    "- sample\n",
    "- take\n",
    "- ....\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Spark is built in Scala, a language derived from Java that uses both the functional programming and OOP paradigms. Spark builds computation by concatenating functions in the DAG.\n",
    "\n",
    "![](./assets/images/spark-flow.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Spark Variables\n",
    "Spark provides two forms of shared variables:\n",
    "- broadcast variables: they reference read-only data that needs to be available on all nodes\n",
    "- accumulators: they can be used to program reductions in an imperative style\n",
    "\n",
    "\n",
    "#### Spark Operations\n",
    "Spark provides two types of operations:\n",
    "- Transformations: these are \"lazy\" operations that only return a result upon \"collect\"\n",
    "- Actions: these are \"non-lazy\" operations that immediately return a result\n",
    "\n",
    "Using lazy operations, we can build a computation graph that only gets executed when we collect the result. This allows Spark to optimize the requested calculation by optimizing the underlying DAG of operations.\n",
    "\n",
    "**Check:** In pairs: each person chooses one of the operations listed above and looks how it works in the documentation. Then you have 2 minutes to explain each other what you have learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Spark runs on a cluster manager and uses a distributed storage system: these can be Hadoop and HDFS, though there are other options.\n",
    "\n",
    "We can also play with Spark in a pseudo-distributed local mode, where the local file system is actually used. Let's do that.\n",
    "\n",
    "(Alternatively, if you're having trouble running the 'dsi-bigdata' vagrant box, you can spin up a cluster running Spark on AWS by following this guide: https://docs.aws.amazon.com/ElasticMapReduce/latest/ReleaseGuide/emr-spark-launch.html. Once your cluster is up, ssh into the master like we did yesterday, and run the `pyspark` shell.\n",
    "\n",
    "Nb - the sample dataset in that tutorial is no longer available. Try reusing the cloudfront log: `sc.textFile(\"s3://us-west-2.elasticmapreduce.samples/cloudfront/data/\")`. Or really go wild with one of these: https://aws.amazon.com/public-data-sets/.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"demo\"></a>\n",
    "## Demo: Spark Map Reduce (15 min)\n",
    "(adapted from: http://spark.apache.org/docs/latest/quick-start.html)\n",
    "\n",
    "For the next part we will use our virtual machine:\n",
    "\n",
    "- In bash, go to the directory with the dsi-bigdata-vm.box and accompanying Vagrantfile\n",
    "- `vagrant up`\n",
    "- `vagrant ssh`\n",
    "- If it requires a password, try `vagrant`\n",
    "- `bigdata_start.sh`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Open a PySpark shell by typing: `pyspark`.\n",
    "\n",
    "![](./assets/images/pysparkshell.png)\n",
    "\n",
    "You should also be able to see an active spark context here: http://10.211.55.101:4040/jobs/\n",
    "\n",
    "![](./assets/images/sparkweb.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's load a text file and perform a few operations:\n",
    "\n",
    "```python\n",
    "textFile = sc.textFile('file:///home/vagrant/data/project_gutenberg/pg11.txt')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have just created an RDD called `textFile`. As you know, RDDs have actions, which return values, and transformations, which return pointers to new RDDs. Let’s start with a few actions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    ">>> textFile.count() # Number of items in this RDD\n",
    "3735\n",
    "\n",
    ">>> textFile.first() # First item in this RDD\n",
    "u\"Project Gutenberg's Alice's Adventures in Wonderland, by Lewis Carroll\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let’s use a transformation. We will use the filter transformation to return a new RDD with a subset of the items in the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    ">>> linesWithAlice = textFile.filter(lambda line: \"Alice\" in line)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notice that the shell returned immediately, since this is a transformation which is lazy. If you type `linesWithAlice`, you should see: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```PythonRDD[10] at RDD at PythonRDD.scala:43```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can chain together transformations and actions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    ">>> textFile.filter(lambda line: \"Alice\" in line).count() # How many lines contain \"Alice\"?\n",
    "396\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "RDD actions and transformations can be used for more complex computations. Let’s say we want to find the line with the most words:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    ">>> textFile.map(lambda line: len(line.split())).reduce(lambda a, b: a if (a > b) else b)\n",
    "18\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's stop here for a second and review the last line.\n",
    "\n",
    "We started with the `textFile` RDD and we first applied the following map: `lambda line: len(line.split())`\n",
    "\n",
    "**Check:** what does this function do?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> Answer: it splits each line into words and counts how many words per line there are\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then we chained to the result of the map the following reduce map: `lambda a, b: a if (a > b) else b`\n",
    "\n",
    "**Check** what does this function do?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Answer: it takes two values and returns the biggest of the two\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We used Python anonymous functions (lambdas), but we can also pass any top-level Python function we want. For example, we’ll define a max function to make this code easier to understand:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    ">>> def max(a, b):\n",
    "...     if a > b:\n",
    "...         return a\n",
    "...     else:\n",
    "...         return b\n",
    "...\n",
    "\n",
    ">>> textFile.map(lambda line: len(line.split())).reduce(max)\n",
    "18\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"guided-practice\"></a>\n",
    "## Guided Practice: Spark Map Reduce (10 min)\n",
    "### Word count\n",
    "\n",
    "**Check:** how did we implement the word count in Hadoop?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> step 1: split into words\n",
    "> step 2: map each word to a pair of (word, 1)\n",
    "> step 3: reduce by key and sum the counts\n",
    "\n",
    "Here is how you would implement it in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    ">>> wordCounts = textFile.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notice that this is a lazy operation. To collect the word counts in our shell, we use the collect action:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    ">>> wordCounts.collect()\n",
    "[(u\"figure!'\", 1), (u'four', 6), (u'hanging', 3), (u'ringlets', 1), (u\"story!'\", 2), (u'Foundation', 14), ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To do this we have used 3 different Spark operations:\n",
    "\n",
    "- map: Return a new distributed dataset formed by passing each element of the source through a function func\n",
    "- flatMap: Similar to map, but each input item can be mapped to 0 or more output items. Using this we are given a single list of all the words in the file instead of a list of lists where each item is the list of words in a given line.\n",
    "- reduceByKey: When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.\n",
    "\n",
    "**Check:** Try modifying the code you have just written.\n",
    "- Try loading a different file\n",
    "- Try sorting the words by count to find the most common words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Caching\n",
    "\n",
    "Spark also supports pulling data sets into a cluster-wide in-memory cache. This is very useful when data is accessed repeatedly, such as when querying a small “hot” dataset or when running an iterative algorithm like [PageRank](https://en.wikipedia.org/wiki/PageRank). As a simple example, let’s mark our `linesWithAlice` dataset to be cached:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    ">>> linesWithAlice.cache()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now try running `linesWithAlice.count()` twice. What happens?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Answer: the first time takes longer than the second time, because the result is cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"ind-practice\"></a>\n",
    "## Independent Practice: Explore the Spark Shell  (15 min)\n",
    "\n",
    "### Pi Estimation\n",
    "Let's estimate the value of π by \"throwing darts\" at a circle. We pick random points in the unit square ((0, 0) to (1,1)) and see how many fall in the unit circle. The fraction should be π / 4. Try changing the value of NUM_SAMPLES and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "def sample(p):\n",
    "    x, y = random(), random()\n",
    "    return 1 if x*x + y*y < 1 else 0\n",
    "\n",
    "NUM_SAMPLES = 100000\n",
    "count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)\n",
    "print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Pi is roughly 3.137640\n",
    "\n",
    "**Check:** What's going on? Why did we just do that?\n",
    "> Answer: We parallelized the generation of NUM_SAMPLES random numbers to estimate the value of Pi. Cool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"conclusion\"></a>\n",
    "## Conclusion: (10 min)\n",
    "\n",
    "- Spark is a cluster computing framework, which runs on a cluster manager and uses a distributed storage system (often Hadoop and HDFS).\n",
    "- Spark is growing rapidly more popular thanks to its increased speed and ease of iteration over MapReduce in some use cases.\n",
    "- It is built with Scala, and includes APIs for Java, Python, Hive and now R.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The evolving Spark ecosystem includes:\n",
    "\n",
    "- **Spark Core:** Contains the basic functionality of Spark; in particular the APIs that define RDDs and the operations and actions that can be undertaken upon them. The rest of Spark's libraries are built on top of the RDD and Spark Core.\n",
    "\n",
    "- **Spark SQL:** Provides APIs for interacting with Spark via the Apache Hive variant of SQL called Hive Query Language (HiveQL). Every database table is represented as an RDD and Spark SQL queries are transformed into Spark operations. For those that are familiar with Hive and HiveQL, Spark can act as a drop-in replacement.\n",
    "\n",
    "- **Spark Streaming:** Enables the processing and manipulation of live streams of data in real time. Many streaming data libraries (such as Apache Storm) exist for handling real-time data. Spark Streaming enables programs to leverage this data similar to how you would interact with a normal RDD as data is flowing in.\n",
    "\n",
    "- **MLlib:** A library of common machine learning algorithms implemented as Spark operations on RDDs. This library contains scalable learning algorithms like classifications, regressions, etc. that require iterative operations across large data sets. The Mahout library, formerly the Big Data machine learning library of choice, will move to Spark for its implementations in the future.\n",
    "\n",
    "- **GraphX:** A collection of algorithms and tools for manipulating graphs and performing parallel graph operations and computations. GraphX extends the RDD API to include operations for manipulating graphs, creating subgraphs, or accessing all vertices in a path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ADDITIONAL RESOURCES\n",
    "\n",
    "- [Spark Slideshare Presentation](http://www.slideshare.net/WillDu1/ten-tools-for-ten-big-data-areas-03apache-spark)\n",
    "- [Quora: what is Apache Spark](https://www.quora.com/What-exactly-is-Apache-Spark-and-how-does-it-work)\n",
    "- [Qubole: Apache Spark Use Cases](https://www.qubole.com/blog/big-data/apache-spark-use-cases)\n",
    "- [Spark Examples](http://spark.apache.org/examples.html)\n",
    "- [Spark getting started](https://districtdatalabs.silvrback.com/getting-started-with-spark-in-python)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
