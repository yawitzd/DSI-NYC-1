{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float: left; margin: 15px;\">\n",
    "# Intro to Big Data\n",
    "Week 10 | Lesson 1.1 \n",
    "\n",
    "![](https://snag.gy/SZOEv2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- recognize big data problems\n",
    "- explain how the map reduce algorithm works\n",
    "- perform a map-reduce on a single node using python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STUDENT PRE-WORK\n",
    "*Before this lesson, you should already be able to:*\n",
    "- Run python scripts from the unix shell\n",
    "- Recall how the `cat` and `sort` unix commands work\n",
    "- [Download VM link here](https://www.dropbox.com/s/egzz6129w90okzf/GA%20DSI%20bigdata%200.9.ova?dl=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LESSON GUIDE\n",
    "| TIMING  | TYPE  | TOPIC  |\n",
    "|:-:|---|---|\n",
    "| 5 min | [Opening](#opening) | Opening |\n",
    "| 20 min | [Introduction](#introduction) | Intro to Big Data |\n",
    "| 20 min | [Guided-practice](#guided-practice) | Guided Practice: Word Count on paper |\n",
    "| 20 min | [Demo](#demo) | MapReduce in python |\n",
    "| 15 min | [Ind-practice](#ind-practice) | Independent practice |\n",
    "| 5 min | [Conclusion](#conclusion) | Conclusion |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"opening\"></a>\n",
    "## Opening (5 min)\n",
    "\n",
    "Congratulations! You've made it to Week 10! This is the last full week of new content, and you already know a ton! Now it's time to think about some major trends in the field, including common tools and problems that data scientists may encounter. It is also time to take the tools you've learned to a new level by scaling up the size of datasets you can tackle. Today we start talking about Big Data!\n",
    "\n",
    "\n",
    "<img src=\"https://snag.gy/mDzP4d.jpg\" style=\"height: 300px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What do you think Big Data is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "*Note:* Big data is a hot topic nowadays. It refers to techniques and tools that allow to store, process and analyze large-scale (multi-terabyte) datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Can you think of any datasets that would be \"big data\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "*Note:* \n",
    "- Facebook social graph\n",
    "- Netflix movie preferences\n",
    "- Large recommender systems\n",
    "- Activity of visitors to a website\n",
    "- Customer activity in a retail store (ie: Target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What challenges exist with such large amounts of data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Note:\n",
    "- processing time\n",
    "- cost\n",
    "- architecture maintenance and setup\n",
    "- hard to visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"introduction\"></a>\n",
    "## Intro to (20 min)\n",
    "\n",
    "Big data is a term used when the data exceeds the processing capacity of typical database. We need a big data analytics when the data grows quickly and we need to uncover hidden patterns, unknown correlations, and other useful information. There are three main features in big data (the 3 \"V\"s):\n",
    "\n",
    "- **Volume**: Large amounts of data\n",
    "- **Variety**: Different types of structured, unstructured, and multi-structured data\n",
    "- **Velocity**: Needs to be analyzed quickly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![3v](./assets/images/3vbigdata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two approaches to Big Data: <br><br>1. High Performance Computing<br><br> 2. Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### High performance Computing\n",
    "Supercomputers are very expensive, very powerful calculators used by researchers to solve complicated math problems.\n",
    "\n",
    "![supercomputer](./assets/images/supercomputer.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Can you think of advantages and disadvantages of this configuration?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "pros:\n",
    "- can perform very complex calculation\n",
    "- centrally controlled\n",
    "- useful for research and defense complicated math problems\n",
    "\n",
    "cons:\n",
    "- expensive\n",
    "- difficult to maintain (self-manged or managed hosting both incur operations overhead)\n",
    "- scalability is bounded (pre-bigdata era:  this would be medium data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cloud computing\n",
    "Instead of one huge machine, what if we got a bunch of (commodity) machines?\n",
    "<center>\n",
    "![commodity hardware](https://snag.gy/fNYgt0.jpg)*Actual AWS Datacenter*</center>\n",
    "\n",
    "**Can you think of advantages and disadvantages of this configuration?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> pros:\n",
    "- Relatively cheaper\n",
    "- Easier to maintain (as a user of the cloud system)\n",
    "- Scalability is unbounded (just add more nodes to the cluster)\n",
    "- Variety of turn-key solutions available through cloud providers\n",
    "\n",
    "> cons:\n",
    "- Complex infrastructure \n",
    "- SME required to leverage lower level resources within infrastructure\n",
    "- Mainly tailored for parallelizable problems\n",
    "- Relatively small cpu power at the lowest level\n",
    "- More I/O between machines\n",
    "\n",
    "The term Big Data refers to the latter case, where commodity hardware with unlimited scalability is used to solve highly parallelizable problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How might we use many computers together to process our data?\n",
    "(In contrast to how we've done it so far on our laptops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parallelism\n",
    "\n",
    "The foundation of Big Data processing, is the idea that a problem can be computed by multiple machines together.  This allows many resources to be used in \"parallel\".\n",
    "\n",
    "<center>\n",
    "![](https://snag.gy/MknIN6.jpg)\n",
    "</center>\n",
    "\n",
    "- Running multiple instances to process data\n",
    "- Data can be subset and solved iteratively \n",
    "- Sub-solutions can be solved independently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Divide and Conquer\n",
    "\n",
    "<img src=\"https://snag.gy/xh2mJA.jpg\">\n",
    "\n",
    "Divide and conquer strategy is a fundamental algorithmic technique for solving a given task, whose steps include:\n",
    "\n",
    "1. split task into subtasks\n",
    "- solve these subtasks independently\n",
    "- recombine the subtask results into a final result\n",
    "\n",
    "The defining characteristic of a problem that is suitable for the divide and conquer approach is that it can be broken down into independent subtasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Map-Reduce\n",
    "\n",
    "<img src=\"https://snag.gy/XBgCOs.jpg\">\n",
    "\n",
    "The term **Map Reduce** indicate a two-phase divide and conquer algorithm initially invented and publicized by Google in 2004. It involves splitting a problem into subtasks and processing these subtasks in parallel and it consists of two phases:\n",
    "\n",
    "1. the **mapper** phase\n",
    "- the **reducer** phase\n",
    "\n",
    "In the **mapper phase**, data is split into chunks and the same computation is performed on each chunk, while in the **reducer phase**, data is aggregated back to produce a final result.\n",
    "\n",
    "Map-reduce uses a functional programming paradigm.  The data processing primitives are mappers and reducers, as we’ve seen.\n",
    "\n",
    "- **mappers** – filter & transform data\n",
    "- **reducers** – aggregate results\n",
    "\n",
    "The functional paradigm is good at describing how to solve a problem, but not very good at describing data manipulations (eg, relational joins)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key Value pairs\n",
    "\n",
    "(note to self -- remake diagram)\n",
    "<img src=\"https://snag.gy/k2FCar.jpg\">\n",
    "\n",
    "Data is passed through the various phases of a **map-reduce pipeline** as key-value pairs.\n",
    "\n",
    "** What python data structures could be used to implement a key value pair? **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- **Dictionary**\n",
    "- **Tuple** of 2 elements\n",
    "- **List** of 2 elements\n",
    "- Named **tuple**\n",
    "\n",
    "To understand map reduce one needs to always keep in mind that data is flowing through a pipeline as key-value pairs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"guided-practice\"></a>\n",
    "## Guided Practice: Word Count on paper (20 min)\n",
    "\n",
    "Let's perform a simple map-reduce in the class, let's find the 10 most common words in the paragraph below.\n",
    "<br><br>\n",
    "<small>\n",
    "    1:  MapReduce is a programming model for large-scale distributed data processing.\n",
    "    2:  It is inspired by the map function and the reduce function of the functional\n",
    "    3:  programming languages such as Lisp, Haskell, or Python. One of the most\n",
    "    4:  important features of MapReduce is that it allows us to hide the low-level\n",
    "    5:  implementation such as message passing or synchronization from users and\n",
    "    6:  allows to split a problem into many partitions. This is a great way to make\n",
    "    7:  trivial parallelization of data processing without any need for\n",
    "    8:  communication between the partitions.\n",
    "    9: MapReduce became main stream because of Apache Hadoop, which is an open\n",
    "    10: source framework that was derived from Google's MapReduce paper.\n",
    "    11: MapReduce allows us to process massive amounts of data in a distributed\n",
    "    12: cluster. In fact, there are many implementations of the MapReduce\n",
    "    13: programming model. Some of them are shown in the following list. It is\n",
    "    14: important to say that MapReduce is not an algorithm; it is just a part\n",
    "    15: of a high-performance infrastructure that provides a lightweight\n",
    "    16: way to run a program in a lot of parallel machines.\n",
    "    17:                from: Practical Data Analysis, Hector Cuesta, 2013\n",
    "    <small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple Map reduce\n",
    "\n",
    "We will do this as follows:\n",
    "- Students will perform the mapper function\n",
    "- Instructor will perform the reducer function\n",
    "\n",
    "Each student will be assigned 1 line of text, and you have to produce a list of key value pairs `(word, 1)` and hand those to the instructor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Check:** what pre-processing should you do your tokens in order to improve the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example: the first line will produce this list:\n",
    "\n",
    "    (MapReduce, 1)\n",
    "    (is, 1)\n",
    "    (a, 1)\n",
    "    (programming, 1)\n",
    "    (model, 1)\n",
    "    (for, 1)\n",
    "    (large-scale, 1)\n",
    "    (distributed, 1)\n",
    "    (data, 1)\n",
    "    (processing, 1)\n",
    "\n",
    "The instructor will then sort them, add up the `1`s for each word and produce the counts.\n",
    "\n",
    "**BEGIN...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Check:** what additional operation did the instructor perform in order to complete the aggregation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Combiners\n",
    "\n",
    "Combiners are intermediate reducers that are performed at node level in a multi node architecture.\n",
    "\n",
    "![](https://snag.gy/lFYfoC.jpg)\n",
    "\n",
    "When data is really large we will distribute it to several mappers running on different machines. Sending a long list of `(word, 1)` pairs to the reducer node is not efficient. We can first aggregate at mapper node level and send the result of the aggregation to the reducer. This is possible because aggregations are associative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's repeat the exercise we did before, with a small change.\n",
    "\n",
    "Let's divide the class in 3 groups, in each group one student will be the combiner, the others will be mappers.\n",
    "- Let's split the text in 3 parts and each group gets one part\n",
    "- Mapper students produce the same list of `(word, 1)` for each line they receive and hand the list to the combiner\n",
    "- Combiner students sort the lists and sum the counts for words that appear in each list\n",
    "- Finally combiner students hand their list of counts to the instructor who will combine the intermediate sums and produce the final result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Congratulations! you have just performed a map-reduce sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Check:** What changed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Check:** Can you think of other aggregation tasks that can be parallelized in this way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Other MapReduce Operations:\n",
    "- count, sum, average\n",
    "- grep, sort, inverted index\n",
    "- graph traversals, some ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"demo\"></a>\n",
    "## MapReduce in python (20 min)\n",
    "\n",
    "Now that we performed map-reduce in person, let's do it in python. Below you can find the code for a simple mapper and reducer that perform the word count.\n",
    "\n",
    "Let's look at them in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mapper.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mapper.py\n",
    "import sys\n",
    "\n",
    "# get text from standard input\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        print '%s\\t%s' % (word, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Check:** What kind of input does `mapper.py` expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**AND** what kind of output does `mapper.py` produce?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reducer.py\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    word, count = line.split('\\t', 1)\n",
    "    \n",
    "    # try to count, if error continue\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Check:** what kind of input does `reducer.py` expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Check:** what kind of output does `reducer.py` produce?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The code can be run with the following command from terminal:\n",
    "\n",
    "```bash\n",
    "cat <input-file> | python mapper.py | sort -k1,1 | python reducer.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Check:** can you explain what each of the 4 steps in the pipe does?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- cat: read the file and streams it line by line\n",
    "- mapper\n",
    "- sort: shuffles the mapper output to sort it by key so that counting is easier\n",
    "- reducer: aggregates by word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Check:** can you find how our previous example *could* be represented in the diagram below?\n",
    "![map reduce word count](./assets/images/word_count_dataflow.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"ind-practice\"></a>\n",
    "## Independent practice (15 min)\n",
    "\n",
    "Now that you have a basic word count set up in python, try doing some of the following:\n",
    "\n",
    "- process a much larger text file (you can download it from internet)\n",
    "> for example a page from wikipedia or a blog article. If you're really ambitious you can take books from project gutemberg.\n",
    "- try to see how the execution time scales with file size\n",
    "- read [this article](http://aadrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html) for some very powerful shell tricks.  Learning to use the shell will save you tons of time munging data on your filesystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"conclusion\"></a>\n",
    "## Conclusion (5 min)\n",
    "In this class we have learned about Big Data and map-reduce. This is an algorithm that works really well for aggregations on very large datasets.\n",
    "\n",
    "**Check:** now that you know how it works can you think of a more specific business application?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> Examples:\n",
    ">\n",
    "- process log files to find security breaches\n",
    "- process medical records to assess spending\n",
    "- process news articles to decide on investments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADDITIONAL RESOURCES\n",
    "\n",
    "- [Top 500 Supercomputers](http://www.top500.org/lists/)\n",
    "- [Google Map Reduce paper](http://research.google.com/archive/mapreduce.html)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
