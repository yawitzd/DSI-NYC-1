{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Lab 2: MLLib\n",
    "\n",
    "In this lab we will explore the MLLib library for machine learning in Spark. The API of this library is very similar to Scikit Learn, and it plays quite nicely with Pandas.\n",
    "\n",
    "This lab follows quite closely [this blog post](https://www.mapr.com/blog/churn-prediction-pyspark-using-mllib-and-ml-packages), so if you're lost you can go have  look there for guidance.\n",
    "\n",
    "Let's start with the usual:\n",
    "    - vagrant up\n",
    "    - vagrant ssh\n",
    "\n",
    "Now you should have access to Jupyter notebook here:\n",
    "\n",
    "    http://10.211.55.101:18888/tree\n",
    "    \n",
    "The problem we will solve is the prediction of [_churn rate_](https://en.wikipedia.org/wiki/Churn_rate), which is a measure of how many customers are lost over a period of time. This is a very important business metric, in particular for large companies like Telecom companies.\n",
    "\n",
    "We will use a dataset provided by [BigML](https://bigml.com/). The data has been copied to your VM, but can also be downloaded [here](https://bml-data.s3.amazonaws.com/churn-bigml-80.csv) and [here](https://bml-data.s3.amazonaws.com/churn-bigml-20.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Disable warnings, set Matplotlib inline plotting and load Pandas package\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "pd.options.display.mpl_style = 'default'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the SparkContext and sqlContext are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.a: Load the data\n",
    "\n",
    "Let's start by loading the data. Since the input is a CSV file we'll need to provide a parser.\n",
    "\n",
    "- Use the sqlContext.read.load function to load the data\n",
    "    - load the bigml-80 file to an RDD called CV_data\n",
    "    - load the bigml-20 file to an RDD called final_test_data\n",
    "    - cache CV_data to speed up things\n",
    "    \n",
    "Note that you can print the schema of the RDD if you want to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.b: Quick look at the data\n",
    "\n",
    "- use the `take` function to take the first 5 lines of the `CV_data` RDD and display them as Pandas dataframe\n",
    "- correct the column types of the RDD\n",
    "- use the `describe` function to have some summary statistics about the training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Sample inspection\n",
    "\n",
    "Not all the features are numeric. `CV_data.dtypes` contains information on the type.\n",
    "\n",
    "- select the features that are either `int` or `double`\n",
    "- use the `sample` function to get a 10% sample of the training RDD\n",
    "- Display a Pandas.scatter_matrix of the sampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Feature selection\n",
    "\n",
    "Column selection on an RDD works differently than in Scikit Learn. For example if we want to drop 2 columns in Spark, we just apply the `.drop(column)` function 2 times.\n",
    "\n",
    "- Drop the following columns:\n",
    "    - State\n",
    "    - Area Code\n",
    "    - Total day charge\n",
    "    - Total eve charge\n",
    "    - Total night charge\n",
    "    - Total intl charge\n",
    "    \n",
    "Also, we can apply a function to a column with the construct:\n",
    "\n",
    "    .withColumn('column_name', function(CV_data['column_name']))\n",
    "    \n",
    "Use it to transform binary string labels to `1.0` or `0.0`. Treat these columns:\n",
    "\n",
    "    - Churn\n",
    "    - International plan\n",
    "    - Voice mail plan\n",
    "\n",
    "You may need these two imports:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "```\n",
    "\n",
    "Also, use the `.cache` function to cache your pipeline results so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, take 5 lines and display them with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Train Decision Tree\n",
    "\n",
    "Time has come to do our first model using MLLib. We will use a decision tree.\n",
    "\n",
    "- [LabeledPoint](https://spark.apache.org/docs/0.8.1/api/mllib/org/apache/spark/mllib/regression/LabeledPoint.html) allows us to represent a data point with features and labels. Map it across the data using a function\n",
    "- `.randomSplit` allows us to split the data in train/test sets. Do an 80/20 split\n",
    "- Train a [DecisionTree](http://spark.apache.org/docs/latest/mllib-decision-tree.html) on the training data\n",
    "- Display the trained model using `print model.toDebugString()`\n",
    "\n",
    "You may need the following imports:\n",
    "\n",
    "```python\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Model valuation\n",
    "\n",
    "\n",
    "The MulticlassMetrics module contains a lot of metrics functions.\n",
    "\n",
    "- Evaluate the model on the test data using `.predict`\n",
    "- Calculate the following metrics:\n",
    "    - Precision of True \n",
    "    - Precision of False\n",
    "    - Recall of True    \n",
    "    - Recall of False   \n",
    "    - F-1 Score         \n",
    "    - Confusion Matrix\n",
    "\n",
    "- Finally, display how many \n",
    "\n",
    "```python\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Cross Validation\n",
    "\n",
    "The [original blog post mentioned above](https://www.mapr.com/blog/churn-prediction-pyspark-using-mllib-and-ml-packages) also contains code to implement cross validation. Try it and see if you understand how it's done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
