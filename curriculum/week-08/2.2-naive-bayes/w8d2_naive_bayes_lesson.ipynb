{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png)  Naive Bayes classifier\n",
    "Week 8 | 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- Describe Naive Bayes\n",
    "- Choose a Naive Bayes implementation based on your use case\n",
    "- Implement a Naive Bayes model through scikit-learn\n",
    "\n",
    "### STUDENT PRE-WORK\n",
    "*Before this lesson, you should already be able to:*\n",
    "- Work with methods in scikit-learn\n",
    "- Conceptually explain the Bayesian posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LESSON GUIDE\n",
    "| Timing | Type | Topic |\n",
    "| --- | --- | --- |\n",
    "| 5 min | [Opening](#opening) | Bayes' theorem and Naive Bayes |\n",
    "| 25 min | [Introduction](#introduction) | The basics of Naive Bayes |\n",
    "| 25 min | [Guided Practice](#Guided)  | Using the Naive Bayes Implementation in Scikit-learn |\n",
    "| 25 min | [Independent Practice](#Indy) | Apply your Naive Bayes on the data |\n",
    "| 5 min |  [Conclusion](#conclusion)| Concluding Remarks |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayes' thereom, again:\n",
    "\n",
    "\n",
    "### $$P\\left(\\;A\\;|\\;B\\;\\right) = \\frac{P\\left(\\;B\\;|\\;A\\;\\right)P\\left(\\;A\\;\\right)}{P(\\;B\\;)}$$\n",
    "\n",
    "\n",
    "### $$P\\left(\\;model\\;|\\;data\\;\\right) = \\frac{P\\left(\\;data\\;|\\;model\\;\\right)P\\left(\\;model\\;\\right)}{P(\\;data\\;)} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Applying Bayes in supervised machine learning\n",
    "\n",
    "> Check: How would you apply this in a machine learning context?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can use this for classification problems.\\* Its canonical use case is spam classification (or text classification generally).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<sub><sup>\\*Or regression. But it doesn't work well.</sub></sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What would our formula look like?\n",
    "\n",
    "Let's say we're trying to predict 419 spam emails. M = 'million', S = 'is spam'.\n",
    "\n",
    "#### $$P\\left(\\;S\\;|\\;M\\;\\right) = \\frac{P\\left(\\;M\\;|\\;S\\;\\right)P\\left(\\;S\\;\\right)}{P(\\;M\\;)} = \\frac{P\\left(\\;M\\;|\\;S\\;\\right)P\\left(\\;S\\;\\right)}{P(\\;M\\;|\\;S)P(\\;S\\;) + P(\\;M\\;|\\;\\neg{S})P(\\;\\neg{S}\\;)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can make some simplifying assumptions. Let's start by assuming an equal chance of spam / not spam. So:\n",
    "\n",
    "### $$P\\left(\\;S\\;|\\;M\\;\\right) = \\frac{P\\left(\\;M\\;|\\;S\\;\\right)}{P(\\;M\\;|\\;S) + P(\\;M\\;|\\;\\neg{S})}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But we'll use more than one feature. Really, we want to see some feature vector $X_1, X_2, ..., X_n$:\n",
    "\n",
    "### $$P\\left(\\;S\\;|\\;X_1, X_2, ..., X_n\\;\\right) = \\frac{P\\left(\\;X_1, X_2, ..., X_n\\;|\\;S\\;\\right)}{P(\\;X_1, X_2, ..., X_n\\;|\\;S) + P(\\;X_1, X_2, ..., X_n\\;|\\;\\neg{S})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since these features can take on different values in each observation, our calculation is really:\n",
    "\n",
    "### $$P\\left(\\;S\\;|\\;X_{1=x1}, X_{2=x2}, ..., X_{n=xn}\\;\\right) = \\frac{P\\left(\\;X_{1=x1}, X_{2=x2}, ..., X_{n=xn}\\;|\\;S\\;\\right)}{P(\\;X_{1=x1}, X_{2=x2}, ..., X_{n=xn}\\;|\\;S) + P(\\;X_{1=x1}, X_{2=x2}, ..., X_{n=xn}\\;|\\;\\neg{S})}$$\n",
    "\n",
    "\n",
    "With a lot of features, calculating their joint probabilities could get hairy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simplify again, naively\n",
    "\n",
    "Joint probabilities are NBD if we *assume independence*: \n",
    "$P\\left(\\;X_{1=x1}, X_{2=x2}, ..., X_{n=xn}\\;|\\;S\\;\\right) = P\\left(\\;X_{1=x1} |\\;S\\;\\right) * P\\left(\\;X_{2=x2} |\\;S\\;\\right) ... P\\left(\\;X_{n=xn} |\\;S\\;\\right)$\n",
    "\n",
    "$$P\\left(\\;S\\;|\\;X_{1=x1}, X_{2=x2}, ..., X_{n=xn}\\;\\right) = \\prod_{i=1}^{n}P(X_i = x_i | \\;S\\;) / C$$\n",
    "\n",
    "Where C is some constant for our marginal probability of those data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### This gives a handy decision function (generalizable to k classes)\n",
    "\n",
    "![](./assets/images/nb_decision_rule.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using our Naive Bayes model\n",
    "\n",
    "How do we code this and instantiate models?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How would you?\n",
    "\n",
    "> Check: With a partner, jot down (pseudo)code for a Naive Bayes classifier. What are the inputs and outputs? How did you calculate probabilities? What implementation wrinkles do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Moving toward a production implementation\n",
    "\n",
    "Possible issues to contend with:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- [Underflow](http://stackoverflow.com/questions/3704570/in-python-small-floats-tending-to-zero). Probabilites may very very small, too small for floating point arithmetic. We can solve by leveraging:\n",
    "\n",
    "$$log(ab) = log\\ a + log\\ b$$\n",
    "\n",
    "$$exp(log\\ x) = x$$\n",
    "\n",
    "So $P_1\\ *\\ P_2\\ ...\\ *\\ P_2 = exp(log\\ P_1 + ... + log\\ P_n)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- '0' probabilities. What if you never saw a feature value in your training data? We can use Laplace smoothing:\n",
    "\n",
    "$$\\hat\\theta_i= \\frac{x_i + \\alpha}{N + \\alpha d}  \\qquad (i=1,\\ldots,d)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Real-valued features. This brings us to *distributions*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The likelihood functions\n",
    "\n",
    "$P\\left(\\;X_{1=x1}, X_{2=x2}, ..., X_{n=xn}\\;|\\;S\\;\\right)$\n",
    "\n",
    "Bayesians tend to talk in terms of distributions of belief. Rather than point estimates of probabilities, we can use distributions.\n",
    "\n",
    "For a binary event, probability can be modeled with the **binomial distribution**.\n",
    "\n",
    "For > 2 discrete outcomes, the **multinomial distribution**.\n",
    "\n",
    "And if features are real-valued? **Gaussian**.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Guided practice: Scikit-learn to the rescue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<a name = \"demo\"></a>\n",
    "### Using the Naive Bayes Implementation in Scikit-learn (15 mins)\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "# Import data into a numpy array\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Y = np.array([1, 1, 1, 2, 2, 2])\n",
    "\n",
    "#Initialize a variable as the Guassian Naive Bayes classifier and fit it with the data\n",
    "clf = GaussianNB()\n",
    "clf.fit(X, Y)\n",
    "GaussianNB()\n",
    "\n",
    "# Predict a few instances\n",
    "print(clf.predict([[-0.8, -1]]))\n",
    "clf_pf = GaussianNB()\n",
    "clf_pf.partial_fit(X, Y, np.unique(Y))\n",
    "GaussianNB()\n",
    "print(clf_pf.predict([[-0.8, -1]]))\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "# Import data into a numpy array\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Y = np.array([1, 1, 1, 2, 2, 2])\n",
    "\n",
    "#Initialize a variable as the Guassian Naive Bayes classifier and fit it with the data\n",
    "clf = GaussianNB()\n",
    "clf.fit(X, Y)\n",
    "GaussianNB()\n",
    "\n",
    "# Predict a few instances\n",
    "print(clf.predict([[-0.8, -1]]))\n",
    "clf_pf = GaussianNB()\n",
    "clf_pf.partial_fit(X, Y, np.unique(Y))\n",
    "GaussianNB()\n",
    "print(clf_pf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name = \"Guided\"></a>\n",
    "## Independent practice: Naive-Bayes classifier with real data (25 mins)\n",
    "\n",
    "We're going to now try our hand at classifying some SPAM.\n",
    "\n",
    "```python\n",
    "# Work here\n",
    "import sklearn import naive_bayes\n",
    "import numpy as np; import csv; import urllib\n",
    "\n",
    "urllink = urllib('https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data')\n",
    "\n",
    "read_csv_data = csv.reader(urllink)\n",
    "\n",
    "```\n",
    "\n",
    "Following with our first example, since we want to use scikit-learn's naive Bayes implementation we'll have to do some juggling to get the numpy array \"just right\".\n",
    "\n",
    "For computing reasons, it makes more sense to define our numpy data matrix beforehand (instead of dynamically iterating the numpy array):\n",
    "\n",
    "```python\n",
    "# Work here - 2 different ways to load up the data\n",
    "# Using Numpy\n",
    "ri = 0\n",
    "for row in read_csv_data:\n",
    "    ri = ri + 1\n",
    "\n",
    "numpy_data_mat = np.array(-1*np.ones((ri, 58), float), object)\n",
    "\n",
    "numpy_iter = 0\n",
    "for ri in read_csv_data:\n",
    "    numpy_data_mat[numpy_iter, :] = numpy.array(ri)\n",
    "    numpy_iter = numpy_iter + 1\n",
    "\n",
    "numpy_data_mat_2 = -1*np.ones_like(numpy_data_mat)\n",
    "\n",
    "# Using Pandas\n",
    "pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data')\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name = \"Indy\"></a>\n",
    "## Apply your Naive Bayes on the data  (25 min)\n",
    "\n",
    "Now we should take the results above and try our hand with Naive Bayes. Which Naive Bayes classifier should we utilize? There are 3 variants (Normal, Bernoulli, Multinomial). Could we do some conversion of the data and try one or the other? How should we think about diagnosing the model performance?\n",
    "\n",
    "Again, we must defer to the docs:\n",
    "\n",
    "- [Docs 1](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n",
    "- [Docs 2](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "- [Docs 3](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html)\n",
    "\n",
    "The differences can be summarized as follows\n",
    "-    ***BernoulliNB*** is designed for binary/boolean features\n",
    "-    The ***multinomial Naive Bayes classifier*** is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as `tf-idf` may also work\n",
    "-    ***GaussianNB*** is designed for continuous features (that can be scaled between 0,1) and is assumed to be normally distributed\n",
    "\n",
    "```python\n",
    "# Work here\n",
    "\n",
    "# We need to separate the features from the target.\n",
    "\n",
    "feature_set = numpy_data_mat[:, :-1]\n",
    "target = numpy_dat_mat[:, -1]\n",
    "\n",
    "classifier1 = MultinomialNB().fit(feature_set, target)\n",
    "\n",
    "# Define several different feature sets, I just dumped everything into the model, but do we get more or better accuracy #based on what set of features we put in? Is more always better?\n",
    "\n",
    "# Discuss... and think about what kind of diagnosis metrics we could utilize for the model\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name = \"conclusion\"></a>\n",
    "## Conclusion (5 min)\n",
    "\n",
    "\n",
    "How does Naive Bayes fit into your toolkit? What are the pros and cons? How do you choose between variants?\n",
    "\n",
    "#### Additional Resources\n",
    "\n",
    "- [An interesting slide from a Stanford MOOC which had a section on Naive Bayes](https://web.stanford.edu/class/cs124/lec/naivebayes.pdf)\n",
    "- [A much more technical paper comparing Naive Bayes to Logistics Regressions](https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf)\n",
    "- [More exposition on Naive Bayes](http://blog.yhat.com/posts/naive-bayes-in-python.html)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
